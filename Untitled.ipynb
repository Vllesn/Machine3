{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validiation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5ae7ead56e36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7198\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validiation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validiation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv as read\n",
    "path = \"D:/AppleStore.csv\"\n",
    "data = read(path, delimiter=\",\")\n",
    "data.head()\n",
    "X = data.values[::, 1:7198]\n",
    "Y = data.values[::, 1:17]\n",
    "from sklearn.cross_validiation import train_test_split as train\n",
    "X_train, X_test, Y_train, Y_test = train(X, Y, test_size=0.6)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "clf.fit(X_train,Y_train)\n",
    "clt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    %matplotlib inline\n",
    "        class LinRegression:\n",
    "        '''Linear Regression.'''\n",
    "        def __init__(self):\n",
    "            self.about = \"Linear Regression by Sergei Bernadsky\"\n",
    "            self.W = [] # model's weights\n",
    "            self.fscaling = False # is feature scaling used\n",
    "            \n",
    "        def cost(self, y_real, y_pred): \n",
    "            # cost function for gradient descent algorithm\n",
    "            return np.sum((y_pred-y_real)**2)/(len(y_real))\n",
    "        \n",
    "        def gradient_descent_step(self, learning_rate, dy, m, n, X_tr):\n",
    "            # one gradient descent step\n",
    "            s = (np.dot(dy.T, X_tr)).reshape(n, 1)\n",
    "            dW = 2*(learning_rate*s/m).reshape(n, 1)\n",
    "            return self.W - dW\n",
    "        \n",
    "        def normalize(self, X):\n",
    "            # normilize X table\n",
    "            for j in range(X.shape[1]):\n",
    "                X[:,j] = X[:,j]/np.max(X[:,j])\n",
    "            return X\n",
    "        \n",
    "        def fit(self, X, y, learning_rate = 0.99, nsteps = 3000, e = 0.000000001,\n",
    "                weight_low = 0, weight_high = 1,\n",
    "                fscaling = False, kweigths = 1, random_state = 0):\n",
    "            # train our Linear Regression model\n",
    "            \n",
    "            np.random.seed(random_state)\n",
    "            X = X.astype(float)\n",
    "            \n",
    "            # Normilize process\n",
    "            if fscaling == True:\n",
    "                X = self.normalize(X)\n",
    "                self.fscaling = True\n",
    "            m = X.shape[0]\n",
    "            # add one's column to X\n",
    "            X = np.hstack( (np.ones(m).reshape(m, 1), X) )\n",
    "            n = X.shape[1]\n",
    "            \n",
    "            # Weights: random initialization\n",
    "            self.W = np.random.randint(low = weight_low, high = weight_high, size=(n, 1))\n",
    "                \n",
    "            y_pred = np.dot(X, self.W)\n",
    "            cost0 = self.cost(y, y_pred)\n",
    "            y = y.reshape(m, 1)\n",
    "            k = 0\n",
    "            \n",
    "            ########## Gradient descent's steps #########\n",
    "            while True:\n",
    "                dy = y_pred - y\n",
    "                W_tmp = self.W\n",
    "                self.W = self.gradient_descent_step(learning_rate, dy, m, n, X)\n",
    "                y_pred = np.dot(X, self.W)\n",
    "                cost1 = self.cost(y, y_pred)\n",
    "                k += 1\n",
    "                if (cost1 > cost0):\n",
    "                    self.W = W_tmp\n",
    "                    break    \n",
    "                    \n",
    "                if ((cost0 - cost1) < e) or (k == nsteps):\n",
    "                    break\n",
    "                    \n",
    "                cost0 = cost1\n",
    "            #############################################\n",
    "            return self.W # return model's weights\n",
    "        \n",
    "        def predict(self, X):\n",
    "            m = X.shape[0]\n",
    "            if self.fscaling == False:\n",
    "                return np.dot( np.hstack( (np.ones(m).reshape(m, 1),\n",
    "                                           X.astype(float)) ) ,\n",
    "                              self.W)\n",
    "            else:\n",
    "                return np.dot( np.hstack( (np.ones(m).reshape(m, 1),\n",
    "                                           self.normalize(X.astype(float))) ),\n",
    "                              self.W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
